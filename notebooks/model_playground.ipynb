{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "import evoVAE.utils.metrics as mt\n",
    "from evoVAE.models.seqVAETest import SeqVAETest\n",
    "from evoVAE.trainer.seqVAE_train import seq_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        # Dataset info\n",
    "        \"dataset\": \"playground\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.1, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAETest\",\n",
    "        \"latent_dims\": 2,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = \"/Users/sebs_mac/OneDrive - The University of Queensland/honours/data/gfp_alns/independent_runs/no_synthetic/ancestors/seqs/\"\n",
    "filepath = DATA_PATH + 'run_14_ancestors.fa'\n",
    "aln = st.read_aln_file(filepath)\n",
    "#aln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, val = train_test_split(aln, test_size=0.2)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "train_dataset = MSA_Dataset(\n",
    "    train[\"encoding\"], train.index, train[\"id\"]\n",
    ")\n",
    "\n",
    "# VALIDATION\n",
    "val_dataset = MSA_Dataset(\n",
    "    val[\"encoding\"], val.index, val[\"id\"]\n",
    ")\n",
    "\n",
    "# DATA LOADERS #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(val_loader))\n",
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length \n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = train_dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * config['AA_count']\n",
    "\n",
    "seq_len, input_dims\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAETest(input_dims=input_dims, latent_dims=config['latent_dims'], hidden_dims=config['hidden_dims'], config=config) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very small nummy data \n",
    "dummy = next(iter(train_loader))[0].float()\n",
    "print(dummy.shape)\n",
    "# reconstruct input, note it has been flattened \n",
    "log_p, z_sample, z_mu, z_logvar = model(dummy)\n",
    "\n",
    "# grab the shape of the input for reshaping\n",
    "orig_shape = log_p.shape[0:-1]\n",
    "\n",
    "# add on extra dim, then make it one-hot encoding shape (obs, seq_len, AA_count)\n",
    "log_p = torch.unsqueeze(log_p, -1)\n",
    "log_p = log_p.view(orig_shape + (-1, config['AA_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data = pd.read_csv('../data/dms_data/GFP_AEQVI_Sarkisyan_2016.csv')\n",
    "subset = mut_data.copy()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, _ = st.encode_and_weight_seqs(mut_data['mutated_sequence'], 0.2, reweight=False)\n",
    "mut_data['encoding'] = encoding\n",
    "mut_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data.to_pickle(\"GFP_AEQVI_Sarkisyan_2016_dms_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, weights = st.encode_and_weight_seqs(subset['mutated_sequence'], 0.2, reweight=True)\n",
    "subset['encoding'] = encoding\n",
    "subset['weights'] = weights\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/dms_data/DMS_substitutions.csv\")\n",
    "metadata = metadata[metadata[\"DMS_id\"].str.contains(\"GFP\")]\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "wild_type = metadata['target_seq'].to_numpy()[0]\n",
    "wild_one_hot = torch.Tensor(st.seq_to_one_hot(wild_type)).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "wild_model_encoding, _, _, _ = model(wild_one_hot)\n",
    "\n",
    "orig_shape = wild_model_encoding.shape[0:-1]\n",
    "\n",
    "wild_model_encoding = torch.unsqueeze(wild_model_encoding, -1)\n",
    "\n",
    "wild_model_encoding = wild_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "\n",
    "\n",
    "# get the wild type encoding \n",
    "wild_model_encoding = wild_model_encoding.squeeze(0)\n",
    "\n",
    "one_hot = wild_one_hot.squeeze(0)\n",
    "wt_prob = mt.seq_log_probability(one_hot, wild_model_encoding)\n",
    "\n",
    "variant_encodings = torch.Tensor(np.stack(subset['encoding'].values))\n",
    "variant_model_outputs, _, _, _ = model(variant_encodings)\n",
    "\n",
    "model_scores =[]\n",
    "for variant, var_one_hot in zip(variant_model_outputs, variant_encodings):\n",
    "    \n",
    "    print(variant.shape)\n",
    "    var_model_encoding = torch.unsqueeze(variant, -1)\n",
    "    print(var_model_encoding.shape)\n",
    "    var_model_encoding = var_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "    var_model_encoding = var_model_encoding.squeeze(0)\n",
    "    log_prob = mt.seq_log_probability(var_one_hot, var_model_encoding)\n",
    "    \n",
    "    # make variant fitness relative to the wild type\n",
    "    model_scores.append(log_prob - wt_prob)\n",
    "    \n",
    "model_scores = pd.Series(model_scores)\n",
    "model_scores, subset['DMS_score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0,0,0])\n",
    "b = torch.Tensor([1,2,3])\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spear_rho, k_recall, ndcg, roc_auc = mt.summary_stats(predictions=model_scores, actual=subset['DMS_score'], actual_binned=subset['DMS_score_bin'])\n",
    "spear_rho, k_recall, ndcg, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the alignment: ../data/test.aln\n",
      "Checking for bad characters: ['B', 'J', 'X', 'Z']\n",
      "Performing one hot encoding\n",
      "Number of seqs: 6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m avgs \u001b[38;5;241m=\u001b[39m calc_average_residue_distribution(encode, GAPPY_PROTEIN_ALPHABET)\n\u001b[1;32m     11\u001b[0m ppm \u001b[38;5;241m=\u001b[39m calc_position_prob_matrix(seqs)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_residue_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_repos/evoVAE/evoVAE/utils/seq_tools.py:316\u001b[0m, in \u001b[0;36mplot_residue_distributions\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    313\u001b[0m         plot_info[aa_index]\u001b[38;5;241m.\u001b[39mappend(residues[aa_index])\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Extract category labels, means, and standard deviations\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResidue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Residue Proportion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.utils.seq_tools import GAPPY_PROTEIN_ALPHABET, calc_position_prob_matrix, calc_mean_seq_embeddings, calc_average_residue_distribution, calc_position_prob_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "seqs = st.read_aln_file(\"../data/test.aln\")\n",
    "encode = calc_mean_seq_embeddings(seqs)\n",
    "avgs = calc_average_residue_distribution(encode, GAPPY_PROTEIN_ALPHABET)\n",
    "ppm = calc_position_prob_matrix(seqs)\n",
    "\n",
    "\n",
    "plot_residue_distributions(encode)\n",
    "#print(ppm)\n",
    "#print()\n",
    "#print(avgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "import evoVAE.utils.metrics as mt\n",
    "from evoVAE.models.seqVAETest import SeqVAETest\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to test new features for a model without having to use the WandB service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        # Dataset info\n",
    "        \"dataset\": \"playground\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.1, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAETest\",\n",
    "        \"latent_dims\": 3,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = \"/Users/sebs_mac/uni_OneDrive/honours/data/gfp_alns/independent_runs/no_synthetic/alns/\"\n",
    "filepath = DATA_PATH + 'GFP_AEQVI_full_04-29-2022_b08_extants_no_syn.aln'\n",
    "aln = st.read_aln_file(filepath)\n",
    "aln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa, _, _ = st.convert_msa_numpy_array(aln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.seq_tools import encode_and_weight_seqs\n",
    "\n",
    "code, weight = encode_and_weight_seqs(aln, 0.2)\n",
    "aln['weight'] = weight\n",
    "aln = aln[:]\n",
    "aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train, val = train_test_split(aln, test_size=0.2)\n",
    "\n",
    "# # TRAINING\n",
    "# train_dataset = MSA_Dataset(\n",
    "#     train[\"encoding\"], train['weight'], train[\"id\"]\n",
    "# )\n",
    "\n",
    "# # VALIDATION\n",
    "# val_dataset = MSA_Dataset(\n",
    "#     val[\"encoding\"], val['weight'], val[\"id\"]\n",
    "# )\n",
    "\n",
    "# # DATA LOADERS #\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# print(len(train_loader), len(val_loader))\n",
    "# #next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length \n",
    "from torch import Tensor\n",
    "from evoVAE.models.seqVAE import SeqVAE\n",
    "import evoVAE.utils.statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = MSA_Dataset(\n",
    "    aln[\"encoding\"], aln['weight'], aln[\"id\"]\n",
    ")\n",
    "\n",
    "# DATA LOADERS #\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * config['AA_count']\n",
    "\n",
    "seq_len, input_dims\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAETest(input_dims=input_dims, latent_dims=config['latent_dims'], hidden_dims=config['hidden_dims'], config=config) \n",
    "\n",
    "\n",
    "def sample_latent_space(model: SeqVAE, data_loader: MSA_Dataset, num_samples: int) -> Tuple[List[str], List[Tensor]]:\n",
    "    \"\"\"\n",
    "    Take a trained model and sample the latent space num_samples many times. This gets the average \n",
    "    latent space representation of that sequence. \n",
    "\n",
    "    Returns:\n",
    "    ids: The ID of each sequence.\n",
    "    x_hats: The model output for each sequence. \n",
    "    \"\"\"\n",
    "    \n",
    "    ids = []\n",
    "    x_hats = []\n",
    "    for encoding, _, id in data_loader:\n",
    "\n",
    "        # get into flat format to pass through the model \n",
    "        encoding = encoding.float()\n",
    "        encoding = torch.flatten(encoding, start_dim=1)\n",
    "\n",
    "        # get encoding and replicate to allow multiple samples from latent space \n",
    "        z_mu, z_logvar = model.encode(encoding)    \n",
    "        z_mu = z_mu.expand(num_samples, z_mu.shape[0], z_mu.shape[1])\n",
    "        z_logvar = z_logvar.expand(num_samples, z_logvar.shape[0], z_logvar.shape[1])\n",
    "        \n",
    "        # pass each sample through the latent space and then average and decode\n",
    "        z_samples = model.reparameterise(z_mu, z_logvar)\n",
    "        mean_z = torch.mean(z_samples, dim=0)\n",
    "        x_hat = model.decode(mean_z)\n",
    "\n",
    "        ids.extend(id)\n",
    "        x_hats.extend(x_hat.detach())\n",
    "        \n",
    "\n",
    "    return ids, x_hats\n",
    "\n",
    "def translate_model_predictions(x_hats: List[Tensor]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the most likely residue for each column in the model reconstruction. \n",
    "\n",
    "    Returns (List[str]):\n",
    "    The string representation of each sequence. \n",
    "    \"\"\"\n",
    "    \n",
    "    reconstructions = []\n",
    "    for x_hat in x_hats:\n",
    "    \n",
    "        # decode the Z sample and get it into a PPM shape\n",
    "        x_hat = x_hat.unsqueeze(-1)\n",
    "        # print(x_hat.shape)\n",
    "        x_hat = x_hat.view(orig_shape)\n",
    "\n",
    "        # Identify most likely residue at each column \n",
    "        indices = x_hat.max(dim=-1).indices.tolist()\n",
    "        recon = \"\".join([st.GAPPY_PROTEIN_ALPHABET[x] for x in indices])\n",
    "        reconstructions.append(recon)\n",
    "\n",
    "    return reconstructions\n",
    "\n",
    "\n",
    "def calc_covariances(ids: List[str], reconstructions: List[Tensor], aln: pd.DataFrame, outfile: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # recons_df = pd.DataFrame({\"id\": ids, \"sequence\": reconstructions})\n",
    "    # recon_msa, _, _ = st.convert_msa_numpy_array(recons_df)\n",
    "    # predicted_covar = stats.pair_wise_covariances(recon_msa)\n",
    "\n",
    "    # # save reconstruction vs actual for visualisation with MSA later\n",
    "    # recons_df[\"sequence\"] = aln[\"sequence\"] \n",
    "    # recons_df[\"reconstructions\"] = reconstructions\n",
    "    # recons_df.to_pickle(outfile + \"_seqs.pkl\")\n",
    "\n",
    "    msa, _, _ = st.convert_msa_numpy_array(aln)\n",
    "    actual_covar = stats.pair_wise_covariances(msa)\n",
    "\n",
    "    return actual_covar, actual_covar\n",
    "\n",
    "\n",
    "num_samples = 5\n",
    "outfile = \"test\"\n",
    "model.eval()\n",
    "ids, x_hats = sample_latent_space(model, loader, num_samples)\n",
    "orig_shape = tuple(aln[\"encoding\"].values[0].shape)\n",
    "reconstructions = translate_model_predictions(x_hats)\n",
    "actual_covar, predicted_covar = calc_covariances(ids, reconstructions, aln, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.scatter(actual_covar, predicted_covar)\n",
    "plt.xlabel(\"MSA covariance\")\n",
    "plt.ylabel(\"Reconstruction covariance\")\n",
    "slope, intercept = np.polyfit(actual_covar, predicted_covar, deg=1)\n",
    "x = np.arange(min([actual_covar.min(), predicted_covar.min()]), max([actual_covar.max(), predicted_covar.max()]) + 0.1, 0.1)\n",
    "regression_line = slope * x + intercept\n",
    "\n",
    "# Calculate correlation coefficient, use [0,1] to not take from diagnonal \n",
    "correlation_coefficient = np.corrcoef(actual_covar, predicted_covar)[\n",
    "    0, 1\n",
    "]\n",
    "\n",
    "# Display correlation value\n",
    "plt.text(\n",
    "    plt.xlim()[0],\n",
    "    plt.ylim()[1],\n",
    "    f\"Pearson's correlation: {correlation_coefficient:.3f}\",\n",
    "    va=\"top\",\n",
    ")\n",
    "\n",
    "plt.title(\"Reconstruction_vs_Actual_MSA_Covariance\")\n",
    "plt.plot(x, regression_line, color=\"red\")\n",
    "\n",
    "filename = outfile + \"_covar.png\"\n",
    "plt.savefig(filename)\n",
    "\n",
    "\n",
    "#print(ids, x_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very small nummy data \n",
    "dummy = next(iter(train_loader))[0].float()\n",
    "print(dummy.shape)\n",
    "# reconstruct input, note it has been flattened \n",
    "log_p, z_sample, z_mu, z_logvar = model(dummy)\n",
    "\n",
    "# grab the shape of the input for reshaping\n",
    "orig_shape = log_p.shape[0:-1]\n",
    "\n",
    "# add on extra dim, then make it one-hot encoding shape (obs, seq_len, AA_count)\n",
    "log_p = torch.unsqueeze(log_p, -1)\n",
    "log_p = log_p.view(orig_shape + (-1, config['AA_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data = pd.read_csv('../data/dms_data/GFP_AEQVI_Sarkisyan_2016.csv')\n",
    "subset = mut_data.copy()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, _ = st.encode_and_weight_seqs(mut_data['mutated_sequence'], 0.2, reweight=False)\n",
    "mut_data['encoding'] = encoding\n",
    "mut_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data.to_pickle(\"GFP_AEQVI_Sarkisyan_2016_dms_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, weights = st.encode_and_weight_seqs(subset['mutated_sequence'], 0.2, reweight=True)\n",
    "subset['encoding'] = encoding\n",
    "subset['weights'] = weights\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/dms_data/DMS_substitutions.csv\")\n",
    "metadata = metadata[metadata[\"DMS_id\"].str.contains(\"GFP\")]\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "wild_type = metadata['target_seq'].to_numpy()[0]\n",
    "wild_one_hot = torch.Tensor(st.seq_to_one_hot(wild_type)).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "wild_model_encoding, _, _, _ = model(wild_one_hot)\n",
    "\n",
    "orig_shape = wild_model_encoding.shape[0:-1]\n",
    "\n",
    "wild_model_encoding = torch.unsqueeze(wild_model_encoding, -1)\n",
    "\n",
    "wild_model_encoding = wild_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "\n",
    "\n",
    "# get the wild type encoding \n",
    "wild_model_encoding = wild_model_encoding.squeeze(0)\n",
    "\n",
    "one_hot = wild_one_hot.squeeze(0)\n",
    "wt_prob = mt.seq_log_probability(one_hot, wild_model_encoding)\n",
    "\n",
    "variant_encodings = torch.Tensor(np.stack(subset['encoding'].values))\n",
    "variant_model_outputs, _, _, _ = model(variant_encodings)\n",
    "\n",
    "model_scores =[]\n",
    "for variant, var_one_hot in zip(variant_model_outputs, variant_encodings):\n",
    "    \n",
    "    print(variant.shape)\n",
    "    var_model_encoding = torch.unsqueeze(variant, -1)\n",
    "    print(var_model_encoding.shape)\n",
    "    var_model_encoding = var_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "    var_model_encoding = var_model_encoding.squeeze(0)\n",
    "    log_prob = mt.seq_log_probability(var_one_hot, var_model_encoding)\n",
    "    \n",
    "    # make variant fitness relative to the wild type\n",
    "    model_scores.append(log_prob - wt_prob)\n",
    "    \n",
    "model_scores = pd.Series(model_scores)\n",
    "model_scores, subset['DMS_score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0,0,0])\n",
    "b = torch.Tensor([1,2,3])\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spear_rho, k_recall, ndcg, roc_auc = mt.summary_stats(predictions=model_scores, actual=subset['DMS_score'], actual_binned=subset['DMS_score_bin'])\n",
    "spear_rho, k_recall, ndcg, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.utils.seq_tools import GAPPY_PROTEIN_ALPHABET, calc_position_prob_matrix, calc_mean_seq_embeddings, calc_position_prob_matrix, create_euclidean_dist_matrix, plot_residue_distributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "seqs = st.read_aln_file(\"../data/test.aln\")\n",
    "encode = calc_mean_seq_embeddings(seqs)\n",
    "ppm = calc_position_prob_matrix(seqs)\n",
    "\n",
    "\n",
    "create_euclidean_dist_matrix(encode, plot=True)\n",
    "\n",
    "\n",
    "plot_residue_distributions(encode)\n",
    "#print(ppm)\n",
    "#print()\n",
    "#print(avgs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebs_mac/git_repos/evoVAE/evoVAE/utils/metrics.py:148: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def hamming_distance(seq1: np.ndarray, seq2: np.ndarray) -> float:\n"
     ]
    }
   ],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "import evoVAE.utils.metrics as mt\n",
    "from evoVAE.models.seqVAETest import SeqVAETest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to test new features for a model without having to use the WandB service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        # Dataset info\n",
    "        \"dataset\": \"playground\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.1, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAETest\",\n",
    "        \"latent_dims\": 3,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the alignment: /Users/sebs_mac/uni_OneDrive/honours/data/gfp_alns/independent_runs/no_synthetic/alns/GFP_AEQVI_full_04-29-2022_b08_ancestors_extants_no_syn.aln\n",
      "Checking for bad characters: ['B', 'J', 'X', 'Z', 'U']\n",
      "Performing one hot encoding\n",
      "Number of seqs: 34106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GFP_AEQVI/1-238</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UniRef100_UPI0011C34247/2-231</td>\n",
       "      <td>VSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniRef100_UPI0011C34247/384-556</td>\n",
       "      <td>VSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKL...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UniRef100_UPI0011C3426C/2-231</td>\n",
       "      <td>VSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UniRef100_UPI0011C3426C/384-556</td>\n",
       "      <td>VSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKL...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0                  GFP_AEQVI/1-238   \n",
       "1    UniRef100_UPI0011C34247/2-231   \n",
       "2  UniRef100_UPI0011C34247/384-556   \n",
       "3    UniRef100_UPI0011C3426C/2-231   \n",
       "4  UniRef100_UPI0011C3426C/384-556   \n",
       "\n",
       "                                            sequence  \\\n",
       "0  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "1  VSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "2  VSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKL...   \n",
       "3  VSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "4  VSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKL...   \n",
       "\n",
       "                                            encoding  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = \"/Users/sebs_mac/uni_OneDrive/honours/data/gfp_alns/independent_runs/no_synthetic/alns/\"\n",
    "filepath = DATA_PATH + 'GFP_AEQVI_full_04-29-2022_b08_ancestors_extants_no_syn.aln'\n",
    "aln = st.read_aln_file(filepath)\n",
    "aln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence weight numpy array created with shape (num_seqs, columns):  (359, 238)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(359, 238)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evoVAE.utils.seq_tools as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange, jit, vectorize\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqs = aln[\"sequence\"].to_numpy()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence weight numpy array created with shape (num_seqs, columns):  (34106, 238)\n"
     ]
    }
   ],
   "source": [
    "msa, _, _ = st.convert_msa_numpy_array(aln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed (new) = 0.574591208999891s\n",
      "Elapsed (serial) = 6.442412040999898s\n",
      "11.212166040989882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "NUM_SEQS = 0 \n",
    "SEQ_LEN = 1\n",
    "\n",
    "@njit()\n",
    "def mini(seq_msa_chunk, aa_type, aa_counts):\n",
    "\n",
    "    weights = np.zeros(seq_msa_chunk.shape[0])\n",
    "\n",
    "    num_type = len(aa_type)\n",
    "    aa_dict = {}\n",
    "    for a, count in zip(aa_type, aa_counts):\n",
    "        aa_dict[a] = count  \n",
    "\n",
    "    for i in range(seq_msa_chunk.shape[NUM_SEQS]):\n",
    "        weights[i] = (1.0 / num_type) * (1.0 / aa_dict[seq_msa_chunk[i]])\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def job(seq_msa_chunk):\n",
    "\n",
    "    aa_type, aa_counts = np.unique(seq_msa_chunk, return_counts=True, axis=0)\n",
    "    weights = mini(seq_msa_chunk, aa_type, aa_counts)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def para_reweight(seq_msa: np.ndarray, n_processes: int):\n",
    "    \n",
    "    #seq_weight = np.zeros(seq_msa.shape)\n",
    "\n",
    "    chunks = [seq_msa[:, col] for col in range(seq_msa.shape[SEQ_LEN])]\n",
    "    results = Parallel(n_jobs=n_processes)(delayed(job)(chunk) for chunk in chunks)\n",
    "\n",
    "    seq_weight = np.column_stack([np.array(r)for r in results])\n",
    "    \n",
    "    tot_weight = np.sum(seq_weight)\n",
    "    seq_weight = seq_weight.sum(axis=1) / tot_weight\n",
    "\n",
    "    return seq_weight\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "seq_weight = para_reweight(msa)\n",
    "end = time.perf_counter()\n",
    "thing = end - start\n",
    "print(\"Elapsed (new) = {}s\".format(thing))\n",
    "\n",
    "start = time.perf_counter()\n",
    "seq_weight_2 = st.position_based_seq_weighting(msa)\n",
    "end = time.perf_counter()\n",
    "print(\"Elapsed (serial) = {}s\".format((end - start)))\n",
    "\n",
    "print((end - start) / thing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 238)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.seq_tools import encode_and_weight_seqs\n",
    "\n",
    "\n",
    "code, weight = encode_and_weight_seqs(aln['sequence'], 0.2)\n",
    "aln['weight'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, val = train_test_split(aln, test_size=0.2)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "train_dataset = MSA_Dataset(\n",
    "    train[\"encoding\"], train['weight'], train[\"id\"]\n",
    ")\n",
    "\n",
    "# VALIDATION\n",
    "val_dataset = MSA_Dataset(\n",
    "    val[\"encoding\"], val['weight'], val[\"id\"]\n",
    ")\n",
    "\n",
    "# DATA LOADERS #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(val_loader))\n",
    "#next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length \n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = train_dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * config['AA_count']\n",
    "\n",
    "seq_len, input_dims\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAETest(input_dims=input_dims, latent_dims=config['latent_dims'], hidden_dims=config['hidden_dims'], config=config) \n",
    "\n",
    "model.eval()\n",
    "for encoding, weights, _ in train_loader:\n",
    "\n",
    "    encoding = encoding.float()\n",
    "    weights = weights.float()\n",
    "    #print(encoding.shape, weight.shape)\n",
    "    modelOutputs = model(encoding)\n",
    "    loss, kl, likelihood = model.loss_function(modelOutputs, encoding, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very small nummy data \n",
    "dummy = next(iter(train_loader))[0].float()\n",
    "print(dummy.shape)\n",
    "# reconstruct input, note it has been flattened \n",
    "log_p, z_sample, z_mu, z_logvar = model(dummy)\n",
    "\n",
    "# grab the shape of the input for reshaping\n",
    "orig_shape = log_p.shape[0:-1]\n",
    "\n",
    "# add on extra dim, then make it one-hot encoding shape (obs, seq_len, AA_count)\n",
    "log_p = torch.unsqueeze(log_p, -1)\n",
    "log_p = log_p.view(orig_shape + (-1, config['AA_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data = pd.read_csv('../data/dms_data/GFP_AEQVI_Sarkisyan_2016.csv')\n",
    "subset = mut_data.copy()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, _ = st.encode_and_weight_seqs(mut_data['mutated_sequence'], 0.2, reweight=False)\n",
    "mut_data['encoding'] = encoding\n",
    "mut_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_data.to_pickle(\"GFP_AEQVI_Sarkisyan_2016_dms_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, weights = st.encode_and_weight_seqs(subset['mutated_sequence'], 0.2, reweight=True)\n",
    "subset['encoding'] = encoding\n",
    "subset['weights'] = weights\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/dms_data/DMS_substitutions.csv\")\n",
    "metadata = metadata[metadata[\"DMS_id\"].str.contains(\"GFP\")]\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "wild_type = metadata['target_seq'].to_numpy()[0]\n",
    "wild_one_hot = torch.Tensor(st.seq_to_one_hot(wild_type)).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "wild_model_encoding, _, _, _ = model(wild_one_hot)\n",
    "\n",
    "orig_shape = wild_model_encoding.shape[0:-1]\n",
    "\n",
    "wild_model_encoding = torch.unsqueeze(wild_model_encoding, -1)\n",
    "\n",
    "wild_model_encoding = wild_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "\n",
    "\n",
    "# get the wild type encoding \n",
    "wild_model_encoding = wild_model_encoding.squeeze(0)\n",
    "\n",
    "one_hot = wild_one_hot.squeeze(0)\n",
    "wt_prob = mt.seq_log_probability(one_hot, wild_model_encoding)\n",
    "\n",
    "variant_encodings = torch.Tensor(np.stack(subset['encoding'].values))\n",
    "variant_model_outputs, _, _, _ = model(variant_encodings)\n",
    "\n",
    "model_scores =[]\n",
    "for variant, var_one_hot in zip(variant_model_outputs, variant_encodings):\n",
    "    \n",
    "    print(variant.shape)\n",
    "    var_model_encoding = torch.unsqueeze(variant, -1)\n",
    "    print(var_model_encoding.shape)\n",
    "    var_model_encoding = var_model_encoding.view(orig_shape + (-1, model.AA_COUNT))\n",
    "    var_model_encoding = var_model_encoding.squeeze(0)\n",
    "    log_prob = mt.seq_log_probability(var_one_hot, var_model_encoding)\n",
    "    \n",
    "    # make variant fitness relative to the wild type\n",
    "    model_scores.append(log_prob - wt_prob)\n",
    "    \n",
    "model_scores = pd.Series(model_scores)\n",
    "model_scores, subset['DMS_score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0,0,0])\n",
    "b = torch.Tensor([1,2,3])\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spear_rho, k_recall, ndcg, roc_auc = mt.summary_stats(predictions=model_scores, actual=subset['DMS_score'], actual_binned=subset['DMS_score_bin'])\n",
    "spear_rho, k_recall, ndcg, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.utils.seq_tools import GAPPY_PROTEIN_ALPHABET, calc_position_prob_matrix, calc_mean_seq_embeddings, calc_position_prob_matrix, create_euclidean_dist_matrix, plot_residue_distributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "seqs = st.read_aln_file(\"../data/test.aln\")\n",
    "encode = calc_mean_seq_embeddings(seqs)\n",
    "ppm = calc_position_prob_matrix(seqs)\n",
    "\n",
    "\n",
    "create_euclidean_dist_matrix(encode, plot=True)\n",
    "\n",
    "\n",
    "plot_residue_distributions(encode)\n",
    "#print(ppm)\n",
    "#print()\n",
    "#print(avgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "all = []\n",
    "\n",
    "with open(\"sample_0.01.output\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = line.strip().split(',')\n",
    "        print(data)\n",
    "        all.append(data)\n",
    "\n",
    "        x.append(float(data[0]))\n",
    "        y.append(float(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(all[-1])\n",
    "error_plus = [(float(all[-1][1]) + float(all[-1][2])/np.sqrt(30000)) for x in range(len(x))]\n",
    "error_neg = [(float(all[-1][1]) - float(all[-1][2])/np.sqrt(30000)) for x in range(len(x))]\n",
    "print(error_plus)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,y)\n",
    "plt.plot(x, [float(all[-1][1]) for x in range(len(x))], label='true mean deviation')\n",
    "plt.plot(x,error_plus, '--r', label=\"true mean + 1SD\")\n",
    "plt.plot(x,error_neg, '--r', label=\"true mean - 1SD\")\n",
    "plt.title(\"GFP ancestors\")\n",
    "plt.ylabel(\"Average Euclidean distance from population residue proportions\", size=12)\n",
    "plt.xlabel(\"Proportion of total population sampled\")\n",
    "plt.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

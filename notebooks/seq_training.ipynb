{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.models.seqVAE import SeqVAE\n",
    "from evoVAE.trainer.seqVAE_train import seq_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"SeqVAE_training\",\n",
    "\n",
    "    # hyperparameters\n",
    "    config = {\n",
    "\n",
    "        # Dataset info\n",
    "        \"dataset\": \"PhoQ\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.9, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 2,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAE\",\n",
    "        \"latent_dims\": 2,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = Path(\"/Users/sebs_mac/OneDrive - The University of Queensland/honours/data/phoQ/uniref90_search/nr65_filtering/odseq_tree/independent_runs/ancestors\")\n",
    "\n",
    "# Gather all the ancestor sequences into a single dataframe \n",
    "trees = []\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file == \"ancestor_trees\":\n",
    "        continue \n",
    "    run = st.read_aln_file(str(DATA_PATH) + \"/\" + file)\n",
    "    run[\"tree\"] = file.split(\"_\")[1]\n",
    "    trees.append(run)\n",
    "\n",
    "ancestors = pd.concat(trees)\n",
    "#ancestors.to_pickle(\"phoQ_ancestors.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, drop N0 and N238 as they come from outgroups \n",
    "print(ancestors.shape)\n",
    "flt_ancestors = ancestors.loc[(ancestors[\"id\"] != \"N0\") & (ancestors[\"id\"] != \"N238\")]\n",
    "print(flt_ancestors.shape)\n",
    "\n",
    "# Then remove non-unique sequences \n",
    "flt_unique_ancestors = flt_ancestors.drop_duplicates(subset=\"sequence\")\n",
    "flt_unique_ancestors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(flt_unique_ancestors, test_size=0.2)\n",
    "\n",
    "# create one-hot encodings and calculate reweightings \n",
    "\n",
    "# TRAINING \n",
    "train_encodings, train_weights = st.encode_and_weight_seqs(train[\"sequence\"],theta=config.seq_theta)\n",
    "train_ids = train[\"id\"].values # just the seq identifiers \n",
    "train_dataset = MSA_Dataset(train_encodings, train_weights, train_ids)\n",
    "\n",
    "# VALIDATION\n",
    "val_encodings, val_weights = st.encode_and_weight_seqs(val[\"sequence\"], theta=config.seq_theta)\n",
    "val_ids = val[\"id\"].values\n",
    "val_dataset = MSA_Dataset(val_encodings, val_weights, val_ids)\n",
    "\n",
    "\n",
    "# DATA LOADERS #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "#next(iter(train_loader))[0].shape,next(iter(train_loader))[1].shape, next(iter(train_loader))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding, weights, id = train_dataset[0]\n",
    "#print(encoding.shape, weights, id)\n",
    "\n",
    "# translation = st.one_hot_to_seq(encoding)\n",
    "# print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length \n",
    "seq_len = train_dataset[0][0].shape[0]\n",
    "input_dims = seq_len * config.AA_count\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAE(input_dims=input_dims, latent_dims=config.latent_dims, hidden_dims=config.hidden_dims, config=config) \n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = seq_train(model, train_loader=train_loader, val_loader=val_loader, device=device, config=config)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.models.seqVAE import SeqVAE\n",
    "from evoVAE.trainer.seqVAE_train import seq_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"SeqVAE_training\",\n",
    "\n",
    "    # hyperparameters\n",
    "    config = {\n",
    "\n",
    "        # Dataset info\n",
    "        \"dataset\": \"PhoQ\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.1, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAE\",\n",
    "        \"latent_dims\": 2,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = Path(\"/Users/sebs_mac/OneDrive - The University of Queensland/honours/data/phoQ/uniref90_search/nr65_filtering/odseq_tree/independent_runs/ancestors\")\n",
    "\n",
    "# Gather all the ancestor sequences into a single dataframe \n",
    "trees = []\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file == \"ancestor_trees\":\n",
    "        continue \n",
    "    run = st.read_aln_file(str(DATA_PATH) + \"/\" + file)\n",
    "    run[\"tree\"] = file.split(\"_\")[1]\n",
    "    trees.append(run)\n",
    "\n",
    "ancestors = pd.concat(trees)\n",
    "anc_encodings, anc_weights = st.encode_and_weight_seqs(ancestors[\"sequence\"],theta=0.2)\n",
    "ancestors[\"weights\"] = anc_weights\n",
    "#ancestors.to_pickle(\"phoQ_ancestors_weights.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, drop N0 and N238 as they come from outgroups \n",
    "print(ancestors.shape)\n",
    "flt_ancestors = ancestors.loc[(ancestors[\"id\"] != \"N0\") & (ancestors[\"id\"] != \"N238\")]\n",
    "print(flt_ancestors.shape)\n",
    "\n",
    "# Then remove non-unique sequences \n",
    "flt_unique_ancestors = flt_ancestors.drop_duplicates(subset=\"sequence\")\n",
    "flt_unique_ancestors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt_unique_ancestors = st.read_aln_file(\"../data/alignments/tiny.aln\")\n",
    "anc_encodings, anc_weights = st.encode_and_weight_seqs(\n",
    "    flt_unique_ancestors[\"sequence\"], theta=config.seq_theta\n",
    ")\n",
    "flt_unique_ancestors[\"weights\"] = anc_weights\n",
    "flt_unique_ancestors[\"encodings\"] = anc_encodings\n",
    "\n",
    "\n",
    "train, val = train_test_split(flt_unique_ancestors, test_size=0.1)\n",
    "\n",
    "# create one-hot encodings and calculate reweightings \n",
    "\n",
    "# TRAINING\n",
    "train_dataset = MSA_Dataset(\n",
    "    train[\"encodings\"], train[\"weights\"], train[\"id\"]\n",
    ")\n",
    "\n",
    "# VALIDATION\n",
    "val_dataset = MSA_Dataset(\n",
    "    val[\"encodings\"], val[\"weights\"], val[\"id\"]\n",
    ")\n",
    "\n",
    "# DATA LOADERS #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(val_loader))\n",
    "next(iter(train_loader))[0].shape,next(iter(train_loader))[1].shape, next(iter(train_loader))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sequence length \n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = train_dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * config.AA_count\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAE(input_dims=input_dims, latent_dims=config.latent_dims, hidden_dims=config.hidden_dims, config=config) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in train_loader:\n",
    "    encoding, weight, name = i\n",
    "\n",
    "    print(encoding.shape)\n",
    "\n",
    "    #encoding = encoding.float()\n",
    "    #output = model.forward(encoding)\n",
    "    #print(encoding.shape, output[0].shape)\n",
    "    #loss, kl, likelihood = model.loss_function(output, encoding)\n",
    "    #print(loss, kl, likelihood)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = seq_train(model, train_loader=train_loader, val_loader=val_loader, device=device, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(size=(2,2))\n",
    "b = torch.rand(size=(2,2))\n",
    "\n",
    "c = a - b\n",
    "print(c)\n",
    "print(c.sum(-1))\n",
    "print(c.sum(dim=tuple(range(1, c.ndim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

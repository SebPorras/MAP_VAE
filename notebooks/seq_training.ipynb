{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.models.seqVAE import SeqVAE\n",
    "from evoVAE.trainer.seqVAE_train import seq_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastian-porras01\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sebs_mac/git_repos/evoVAE/notebooks/wandb/run-20240326_103541-qsd655zh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sebastian-porras01/SeqVAE_training/runs/qsd655zh/workspace' target=\"_blank\">fiery-hill-2</a></strong> to <a href='https://wandb.ai/sebastian-porras01/SeqVAE_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sebastian-porras01/SeqVAE_training' target=\"_blank\">https://wandb.ai/sebastian-porras01/SeqVAE_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sebastian-porras01/SeqVAE_training/runs/qsd655zh/workspace' target=\"_blank\">https://wandb.ai/sebastian-porras01/SeqVAE_training/runs/qsd655zh/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"SeqVAE_training\",\n",
    "\n",
    "    # hyperparameters\n",
    "    config = {\n",
    "\n",
    "        # Dataset info\n",
    "        \"dataset\": \"PhoQ\",\n",
    "        \"seq_theta\": 0.2, # reweighting \n",
    "        \"AA_count\": 21, # standard AA + gap\n",
    "        \n",
    "        # ADAM \n",
    "        \"learning_rate\": 1e-5, # ADAM\n",
    "        \"weight_decay\": 0.01, # ADAM\n",
    "\n",
    "        # Hidden units \n",
    "        \"momentum\": 0.9, \n",
    "        \"dropout\": 0.5,\n",
    "\n",
    "        # Training loop \n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 2,\n",
    "        \"max_norm\": 1.0, # gradient clipping\n",
    "        \n",
    "        # Model info\n",
    "        \"architecture\": \"SeqVAE\",\n",
    "        \"latent_dims\": 2,\n",
    "        \"hidden_dims\": [32, 16],\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the sequences and calculating weights\n",
      "The sequence encoding tensor has size: torch.Size([2, 5, 21])\n",
      "The sequence weight array has size: (2,)\n",
      "\n",
      "Encoding the sequences and calculating weights\n",
      "The sequence encoding tensor has size: torch.Size([1, 5, 21])\n",
      "The sequence weight array has size: (1,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read in the datasets and create train and validation sets \n",
    "alns: pd.DataFrame = st.read_aln_file(\"../data/alignments/tiny.aln\")\n",
    "train, val = train_test_split(alns, test_size=0.2)\n",
    "\n",
    "# create one-hot encodings and calculate reweightings \n",
    "\n",
    "# TRAINING \n",
    "train_encodings, train_weights = st.encode_and_weight_seqs(train[\"sequence\"],theta=config.seq_theta)\n",
    "train_ids = train[\"id\"].values # just the seq identifiers \n",
    "train_dataset = MSA_Dataset(train_encodings, train_weights, train_ids)\n",
    "\n",
    "# VALIDATION\n",
    "val_encodings, val_weights = st.encode_and_weight_seqs(val[\"sequence\"], theta=config.seq_theta)\n",
    "val_ids = val[\"id\"].values\n",
    "val_dataset = MSA_Dataset(val_encodings, val_weights, val_ids)\n",
    "\n",
    "\n",
    "# DATA LOADERS #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "#next(iter(train_loader))[0].shape,next(iter(train_loader))[1].shape, next(iter(train_loader))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding, weights, id = train_dataset[0]\n",
    "#print(encoding.shape, weights, id)\n",
    "\n",
    "# translation = st.one_hot_to_seq(encoding)\n",
    "# print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=105, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (z_mu_sampler): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (z_logvar_sampler): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (upscale_z): Linear(in_features=2, out_features=32, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Linear(in_features=16, out_features=105, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the sequence length \n",
    "seq_len = train_dataset[0][0].shape[0]\n",
    "input_dims = seq_len * config.AA_count\n",
    "\n",
    "# use preset structure for hidden dimensions \n",
    "model = SeqVAE(input_dims=input_dims, latent_dims=config.latent_dims, hidden_dims=config.hidden_dims, config=config) \n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebs_mac/miniconda3/envs/embed/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "trained_model = seq_train(model, train_loader=train_loader, val_loader=val_loader, device=device, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_ELBO</td><td>▁</td></tr><tr><td>epoch_Gauss_likelihood</td><td>▁</td></tr><tr><td>epoch_KLD</td><td>▁</td></tr><tr><td>epoch_val_ELBO</td><td>▁</td></tr><tr><td>epoch_val_Gauss_likelihood</td><td>▁</td></tr><tr><td>epoch_val_KLD</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_ELBO</td><td>654.53662</td></tr><tr><td>epoch_Gauss_likelihood</td><td>-654.61841</td></tr><tr><td>epoch_KLD</td><td>-0.0818</td></tr><tr><td>epoch_val_ELBO</td><td>607.16467</td></tr><tr><td>epoch_val_Gauss_likelihood</td><td>-607.3479</td></tr><tr><td>epoch_val_KLD</td><td>-0.18321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-hill-2</strong> at: <a href='https://wandb.ai/sebastian-porras01/SeqVAE_training/runs/qsd655zh/workspace' target=\"_blank\">https://wandb.ai/sebastian-porras01/SeqVAE_training/runs/qsd655zh/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_103541-qsd655zh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "from evoVAE.models.seqVAETest import SeqVAETest\n",
    "import evoVAE.utils.statistics as stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aln = pd.read_pickle(\"/Users/sebs_mac/uni_OneDrive/honours/data/gb1/encoded_weighted/gb1_extants_encoded_weighted_no_dupes.pkl\")\n",
    "#test_aln = pd.read_pickle(\"../data/gb1/gb1_ancestors_extants_encoded_weighted_no_dupes.pkl\")\n",
    "test_aln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = MSA_Dataset(test_aln[\"encoding\"], test_aln[\"weights\"], test_aln[\"id\"])\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=False\n",
    ")\n",
    "\n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = train_dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * 21\n",
    "\n",
    "\n",
    "config={\n",
    "            # Dataset info\n",
    "            \"alignment\": \"tets\",\n",
    "            \"seq_theta\": 0.2,  # reweighting\n",
    "            \"AA_count\": 21,  # standard AA + gap\n",
    "            \"test_split\": 0.2,\n",
    "            \"max_mutation\": 4,  # how many mutations the model will test up to\n",
    "            # ADAM\n",
    "            \"learning_rate\": 1e-2,  # ADAM\n",
    "            \"weight_decay\": 1e-4,  # ADAM\n",
    "            # Hidden units\n",
    "            \"momentum\": None,\n",
    "            \"dropout\": None,\n",
    "            # Training loop\n",
    "            \"epochs\": 500,\n",
    "            \"batch_size\": 128,\n",
    "            \"max_norm\": 10,  # gradient clipping\n",
    "            \"patience\": 3,\n",
    "            # Model info - default settings\n",
    "            \"architecture\": f\"SeqVAE_0.25_ancestors_R\",\n",
    "            \"latent_dims\": 2,\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            # DMS data\n",
    "            \"dms_file\": \"../data/SPG1_STRSG_Wu_2016.pkl\",\n",
    "            \"dms_metadata\": \"../data/DMS_substitutions.csv\",\n",
    "            \"dms_id\": \"SPG1_STRSG_Wu_2016\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LEN = 0\n",
    "BATCH_ZERO = 0\n",
    "SEQ_ZERO = 0\n",
    "seq_len = train_dataset[BATCH_ZERO][SEQ_ZERO].shape[SEQ_LEN]\n",
    "input_dims = seq_len * 21\n",
    "\n",
    "\n",
    "gb1_good = \"/Users/sebs_mac/git_repos/evoVAE/notebooks/gb1_ancestors_extants_encoded_weighted_no_dupes_model_state.pt\"\n",
    "model = SeqVAETest(input_dims, 3, hidden_dims=config[\"hidden_dims\"], config=config)\n",
    "model.load_state_dict(torch.load(gb1_good))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 10\n",
    "ids = []\n",
    "x_hats = []\n",
    "model.eval()\n",
    "for encoding, _, id in data_loader:\n",
    "\n",
    "    # get into flat format to pass through the model\n",
    "    encoding = encoding.float()\n",
    "    encoding = torch.flatten(encoding, start_dim=1)\n",
    "\n",
    "    # get encoding and replicate to allow multiple samples from latent space\n",
    "    z_mu, z_logvar = model.encode(encoding)\n",
    "    z_mu = z_mu.expand(num_samples, z_mu.shape[0], z_mu.shape[1])\n",
    "    z_logvar = z_logvar.expand(num_samples, z_logvar.shape[0], z_logvar.shape[1])\n",
    "\n",
    "    # pass each sample through the latent space and then average and decode\n",
    "    z_samples = model.reparameterise(z_mu, z_logvar)\n",
    "    mean_z = torch.mean(z_samples, dim=0)\n",
    "    x_hat = model.decode(mean_z)\n",
    "\n",
    "    ids.extend(id)\n",
    "    x_hats.extend(x_hat.detach())\n",
    "\n",
    "orig_shape = tuple(test_aln[\"encoding\"].values[0].shape)\n",
    "reconstructions = []\n",
    "for x_hat in x_hats:\n",
    "\n",
    "    # decode the Z sample and get it into a PPM shape\n",
    "    x_hat = x_hat.unsqueeze(-1)\n",
    "    # print(x_hat.shape)\n",
    "    x_hat = x_hat.view(orig_shape)\n",
    "\n",
    "    # Identify most likely residue at each column\n",
    "    indices = x_hat.max(dim=-1).indices.tolist()\n",
    "    recon = \"\".join([st.GAPPY_PROTEIN_ALPHABET[x] for x in indices])\n",
    "    reconstructions.append(recon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(ids[i])\n",
    "    print(\"actual\", test_aln[test_aln[\"id\"] == ids[i]][\"sequence\"].values[0])\n",
    "    print(\"recon\", reconstructions[i])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa, _, _ = st.convert_msa_numpy_array(test_aln)\n",
    "thing = stats.pair_wise_covariances_parallel(msa, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_df = pd.DataFrame({\"id\": ids, \"sequence\": reconstructions})\n",
    "recons_df.head()\n",
    "\n",
    "recon_msa, _, _ = st.convert_msa_numpy_array(recons_df)\n",
    "thing2 = stats.pair_wise_covariances_parallel(recon_msa, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(thing, thing2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "correlation_coefficient = np.corrcoef(thing, thing2)\n",
    "correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gb1_corv_data = pd.read_csv(\"../data/gb1/covar/ancestors_extants_gb1_msa_cov.csv\")\n",
    "gb1_corv_data.head()\n",
    "\n",
    "\n",
    "plt.scatter(gb1_corv_data['actual'], gb1_corv_data['prediction'])\n",
    "plt.xlabel(\"MSA covariance\")\n",
    "plt.ylabel(\"VAE covariance\")\n",
    "slope, intercept = np.polyfit(gb1_corv_data['actual'], gb1_corv_data['prediction'], 1)\n",
    "x = np.arange(min(gb1_corv_data['actual']),max(gb1_corv_data['actual']) + 0.1, 0.1)\n",
    "regression_line = slope * x + intercept\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation_coefficient = np.corrcoef(gb1_corv_data['actual'], gb1_corv_data['prediction'])[0, 1]\n",
    "\n",
    "# Display correlation value\n",
    "plt.text(plt.xlim()[0], plt.ylim()[1], f'Pearson\\'s correlation: {correlation_coefficient:.2f}', va='top')\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(x, regression_line, color='red')\n",
    "plt.title(\"GB1 3000 random extants/ancestors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

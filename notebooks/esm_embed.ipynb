{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import esm\n",
    "import evoVAE.utils.seq_tools as st\n",
    "from evoVAE.utils.datasets import MSA_Dataset\n",
    "from evoVAE.models.seqVAE import SeqVAE\n",
    "import yaml\n",
    "import evoVAE.utils.visualisation as vs \n",
    "from sklearn.linear_model import LassoLars, Ridge\n",
    "from typing import Tuple\n",
    "dms_path = \"/Users/sebs_mac/git_repos/dms_data/DMS_ProteinGym_substitutions/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the ESM2 package to get sequence embeddings and then fits a regressor to this data\n",
    "\n",
    "I have to use the unaligned sequences because gaps are not included as tokens. \n",
    "\n",
    "First I'm going to use the MAFG variant dataset\n",
    "\n",
    "reference:\n",
    "ESM GitHub: https://github.com/facebookresearch/esm\n",
    "\n",
    "Supervised learning pipeline: https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb#scrollTo=8MpZ87vEbo4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_embeddings(model, alphabet, batch_converter, data, model_layer=-1) -> pd.DataFrame:\n",
    "    \"\"\"Get mean representation of sequence\"\"\"\n",
    "\n",
    "    id_seq_pair = [(id, seq) for id, seq in zip(data[\"mutant\"], data[\"mutated_sequence\"])]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(id_seq_pair)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[model_layer])\n",
    "    token_representations = results[\"representations\"][model_layer]\n",
    "    print(\"Token shape\")\n",
    "    # (num_variants, seq_len_with_start/end_tokens, residue embedding length)\n",
    "    print(token_representations.shape)\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1. Last token is special token that is ignored as well\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "    embeddings = pd.DataFrame({\"mutant\": batch_labels, \"embedding\": sequence_representations})\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def perform_grid_search(train_data, test_data, classifier_classes: list, \n",
    "                        grid_parameters: list, pipeline: Pipeline, cv=10) -> Tuple[list, list]:\n",
    "    result_list = []\n",
    "    grid_list = []\n",
    "    for cls_name, param_grid in zip(classifier_classes, grid_parameters):\n",
    "        print(cls_name)\n",
    "        grid = GridSearchCV(\n",
    "            estimator = pipeline,\n",
    "            param_grid = param_grid,\n",
    "            scoring = 'r2',\n",
    "            verbose = 0,\n",
    "            cv=cv,\n",
    "            n_jobs = -1 # use all available cores\n",
    "        )\n",
    "\n",
    "        grid.fit(train_data, test_data)\n",
    "        result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
    "        grid_list.append(grid)\n",
    "\n",
    "    return result_list, grid_list\n",
    "\n",
    "def setup_loader(data, device):\n",
    "    \n",
    "    dataset = MSA_Dataset(\n",
    "        data[\"one_hot\"],\n",
    "        np.arange(len(data[\"one_hot\"])),\n",
    "        data[\"mutant\"],\n",
    "        device=device,      \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    return loader\n",
    "\n",
    "def instantiate_model(seq_len, device, state_dict, settings):\n",
    "    \n",
    "    model = SeqVAE(dim_latent_vars=settings[\"latent_dims\"],\n",
    "                dim_msa_vars= (seq_len * 21), \n",
    "                num_hidden_units=settings[\"hidden_dims\"],\n",
    "                settings=settings,\n",
    "                num_aa_type=settings[\"AA_count\"])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(state_dict, map_location=device))\n",
    "    return model \n",
    "\n",
    "\n",
    "def instantiate_top_models_and_pipe():\n",
    "    ridge_grid = [\n",
    "        {\n",
    "            'model': [Ridge()],\n",
    "            'model__alpha': np.logspace(-6, 6, 1000),\n",
    "            'model__solver': [\"auto\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    lasso_grid = [\n",
    "        {\n",
    "            'model': [LassoLars()],\n",
    "            'model__alpha': np.logspace(-6, 6, 1000),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    cls_list = [Ridge, LassoLars]\n",
    "    param_grid_list = [ridge_grid, lasso_grid]\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps = [\n",
    "            ('model', 'passthrough')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return cls_list, param_grid_list, pipe\n",
    "\n",
    "def one_hot_top_model(train_data, test_data):\n",
    "    xs_train = np.array([st.seq_to_one_hot(x).flatten() for x in  train_data[\"mutated_sequence\"]])\n",
    "    ys_train = np.array([float(y) for y in train_data[\"DMS_score\"]])\n",
    "    xs_test = np.array([st.seq_to_one_hot(x).flatten() for x in  test_data[\"mutated_sequence\"]])\n",
    "    ys_test = np.array([float(y) for y in test_data[\"DMS_score\"]])\n",
    "\n",
    "    cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "    results, grids = perform_grid_search(xs_train, ys_train, cls_list, param_grid_list, pipe)\n",
    "    for grid in grids:\n",
    "        print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "        print()\n",
    "        preds = grid.predict(xs_test)\n",
    "        print(f'{scipy.stats.spearmanr(ys_test, preds)}')\n",
    "        print('\\n', '-' * 80, '\\n')\n",
    "\n",
    "    return results, grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafg_variants = pd.read_csv(dms_path + \"MAFG_MOUSE_Tsuboyama_2023_1K1V.csv\")\n",
    "mafg_variants[\"one_hot\"] = mafg_variants[\"mutated_sequence\"].apply(st.seq_to_one_hot)\n",
    "mafg_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafg_train, mafg_test = train_test_split(mafg_variants, train_size=0.8, random_state=42)\n",
    "mafg_train.shape, mafg_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM2\n",
    "\n",
    "Using the smallest model with 8M parameters, 6 layers trained on UniRef50.\n",
    "Has 320 embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET MEAN EMBEDDINGS ###\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "train_mafg_esm = get_transformer_embeddings(model, alphabet, batch_converter, mafg_train, model_layer=6)\n",
    "mafg_train = mafg_train.merge(train_mafg_esm, on=\"mutant\")\n",
    "\n",
    "test_mafg_esm = get_transformer_embeddings(model, alphabet, batch_converter, mafg_test, model_layer=6)\n",
    "mafg_test = mafg_test.merge(test_mafg_esm, on=\"mutant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn into arrays for learning/visualisation ###\n",
    "\n",
    "\n",
    "mafg_Xs_train = np.stack(mafg_train[\"embedding\"])\n",
    "\n",
    "mafg_ys_train = np.array([float(y) for y in mafg_train[\"DMS_score\"]])\n",
    "\n",
    "mafg_Xs_test = np.stack(mafg_train[\"embedding\"])\n",
    "\n",
    "mafg_ys_test = np.array([float(y) for y in mafg_train[\"DMS_score\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise what information the embeddings have captured ###\n",
    "num_pca_components = 50\n",
    "pca = PCA(num_pca_components)\n",
    "Xs_train_pca = pca.fit_transform(mafg_Xs_train)\n",
    "\n",
    "fig_dims = (7, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sc = ax.scatter(Xs_train_pca[:,0], Xs_train_pca[:,1], c=mafg_ys_train, marker='.')\n",
    "ax.set_xlabel('PCA first principal component')\n",
    "ax.set_ylabel('PCA second principal component')\n",
    "plt.colorbar(sc, label='Variant Effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform grid search for Lasso and Ridge regression ### \n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(mafg_Xs_train, mafg_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(mafg_Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(mafg_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')\n",
    "\n",
    "# Ridge results\n",
    "#results[0].sort_values('rank_test_score')[:5]\n",
    "# LassoLARS results\n",
    "#results[1].sort_values('rank_test_score')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "\n",
    "mafg_oh_xs_train = np.array([st.seq_to_one_hot(x).flatten() for x in  mafg_train[\"mutated_sequence\"]])\n",
    "print(mafg_oh_xs_train.shape)\n",
    "mafg_oh_ys_train = np.array([float(y) for y in mafg_train[\"DMS_score\"]])\n",
    "print(mafg_oh_ys_train.shape)\n",
    "\n",
    "mafg_oh_xs_test = np.array([st.seq_to_one_hot(x).flatten() for x in  mafg_test[\"mutated_sequence\"]])\n",
    "print(mafg_oh_xs_test.shape)\n",
    "mafg_oh_ys_test = np.array([float(y) for y in mafg_test[\"DMS_score\"]])\n",
    "print(mafg_oh_ys_test.shape)\n",
    "\n",
    "results, grids = perform_grid_search(mafg_oh_xs_train, mafg_oh_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(mafg_oh_xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(mafg_oh_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')\n",
    "\n",
    "# Ridge results\n",
    "#results[0].sort_values('rank_test_score')[:5]\n",
    "# LassoLARS results\n",
    "#results[1].sort_values('rank_test_score')[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE-MAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_vis(xs, ys, title, num_pca_components = 2):\n",
    "    ### Visualise the a PCA of the VAE latent space\n",
    "    pca = PCA(num_pca_components)\n",
    "    Xs_train_pca = pca.fit_transform(xs)\n",
    "\n",
    "    fig_dims = (7, 6)\n",
    "    fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    sc = ax.scatter(Xs_train_pca[:,0], Xs_train_pca[:,1], c=ys, marker='.')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(f'PCA1 ({round(pca.explained_variance_[0] * 100, 2)}%)')\n",
    "    ax.set_ylabel(f'PCA2 ({round(pca.explained_variance_[1] * 100, 2)}%)')\n",
    "    plt.colorbar(sc, label='Variant Effect')\n",
    "\n",
    "def get_vae_latent(model, loader, orig_data):\n",
    "    \n",
    "    latent = vs.get_mu(model, loader)\n",
    "    latent.rename(columns={\"id\": \"mutant\"}, inplace=True)\n",
    "    latent = latent.merge(orig_data[[\"mutant\", \"DMS_score\"]], on=\"mutant\")\n",
    "\n",
    "    return latent\n",
    "\n",
    "def create_vae_data(model, loader, orig_data):\n",
    "\n",
    "    latent = get_vae_latent(model, loader, orig_data)    \n",
    "    Xs_train = np.stack(latent[\"mu\"])\n",
    "\n",
    "    #print(\"Xs\")\n",
    "    #print(Xs_train.shape)\n",
    "    ys_train = np.array([float(y) for y in latent[\"DMS_score\"]])\n",
    "    ##print(\"ys\")\n",
    "    #print(ys_train.shape)\n",
    "\n",
    "    return Xs_train, ys_train\n",
    "\n",
    "\n",
    "def train_and_fit_vae_top_model(train_data, test_data, states, labels, protein, cls_list, param_grid_list, pipe, settings):\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "    train_loader = setup_loader(train_data, device)\n",
    "    test_loader = setup_loader(test_data, device)\n",
    "\n",
    "    for state, label in zip(states, labels): \n",
    "        print(label)\n",
    "        seq_len = len(train_data[\"mutated_sequence\"].values[0])\n",
    "        model = instantiate_model(seq_len, device, state, settings)\n",
    "\n",
    "        Xs_train, ys_train = create_vae_data(model, train_loader, train_data)\n",
    "        Xs_test,  ys_test = create_vae_data(model, test_loader, test_data)\n",
    "\n",
    "        pca_vis(Xs_train, ys_train, f\"{protein} {label} model\")\n",
    "\n",
    "        results, grids = perform_grid_search(Xs_train, ys_train, cls_list, param_grid_list, pipe)\n",
    "        for grid in grids:\n",
    "            print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "            print()\n",
    "            preds = grid.predict(Xs_test)\n",
    "            print(f'{scipy.stats.spearmanr(ys_test, preds)}')\n",
    "            print('\\n', '-' * 80, '\\n')\n",
    "\n",
    "    return results, grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAFG ancestors/extant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dummy config with correct dimensions \n",
    "with open(\"../data/dummy_config_50_latent.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "a_state_dict = \"/Users/sebs_mac/50_latent/mafg_a/mafg_a_r1_fold_1_model_state.pt\"\n",
    "e_state_dict = \"/Users/sebs_mac/50_latent/mafg_e/mafg_e_r1_fold_1_model_state.pt\"\n",
    "states = [a_state_dict, e_state_dict]\n",
    "labels = [\"Ancestors\", \"Extants\"]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(mafg_train, mafg_test, states, labels, \"MAFG\",\n",
    "                                              cls_list, param_grid_list, pipe, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAFG clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dummy_config.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "data_path = \"/Users/sebs_mac/uni_OneDrive/honours/data/seb_clustering_results/mafg_clusters/\"\n",
    "\n",
    "labels = [0.0, 0.05, 0.1, 0.15]\n",
    "\n",
    "states = [data_path + f\"mafg_{prop}/mafg_{prop}_r1/mafg_{prop}_r1_model_state.pt\" for prop in labels]\n",
    "results, grids = train_and_fit_vae_top_model(mafg_train, mafg_test, states, labels, \"MAFG\",\n",
    "                                               cls_list, param_grid_list, pipe, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn4_variants = pd.read_csv(dms_path + \"GCN4_YEAST_Staller_2018.csv\")\n",
    "gcn4_variants[\"one_hot\"] = gcn4_variants[\"mutated_sequence\"].apply(st.seq_to_one_hot)\n",
    "gcn4_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn4_train, gcn4_test = train_test_split(gcn4_variants, train_size=0.8, random_state=42)\n",
    "gcn4_train.shape, gcn4_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnc4_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "gnc4_model.eval()\n",
    "train_gcn4_esm = get_transformer_embeddings(gnc4_model, alphabet, batch_converter, gcn4_train, model_layer=6)\n",
    "gcn4_train = gcn4_train.merge(train_gcn4_esm, on=\"mutant\")\n",
    "\n",
    "test_gcn4_esm = get_transformer_embeddings(gnc4_model, alphabet, batch_converter, gcn4_test, model_layer=6)\n",
    "gcn4_test = gcn4_test.merge(test_gcn4_esm, on=\"mutant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn into arrays for learning/visualisation ###\n",
    "\n",
    "print(\"Train data created\")\n",
    "gcn4_Xs_train = np.stack(gcn4_train[\"embedding\"])\n",
    "gcn4_ys_train = np.array([float(y) for y in gcn4_train[\"DMS_score\"]])\n",
    "\n",
    "print(\"Test data created\")\n",
    "gcn4_Xs_test = np.stack(gcn4_train[\"embedding\"])\n",
    "gcn4_ys_test = np.array([float(y) for y in gcn4_train[\"DMS_score\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise what information the embeddings have captured ###\n",
    "num_pca_components = 50\n",
    "pca = PCA(num_pca_components)\n",
    "Xs_train_pca = pca.fit_transform(gcn4_Xs_train)\n",
    "\n",
    "fig_dims = (7, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sc = ax.scatter(Xs_train_pca[:,0], Xs_train_pca[:,1], c=gcn4_ys_train, marker='.')\n",
    "ax.set_xlabel('PCA first principal component')\n",
    "ax.set_ylabel('PCA second principal component')\n",
    "plt.colorbar(sc, label='Variant Effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(gcn4_Xs_train, gcn4_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(gcn4_Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(gcn4_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn4_oh_xs_train = np.array([st.seq_to_one_hot(x).flatten() for x in  gcn4_train[\"mutated_sequence\"]])\n",
    "gcn4_oh_ys_train = np.array([float(y) for y in gcn4_train[\"DMS_score\"]])\n",
    "\n",
    "gcn4_oh_xs_test = np.array([st.seq_to_one_hot(x).flatten() for x in  gcn4_test[\"mutated_sequence\"]])\n",
    "gcn4_oh_ys_test = np.array([float(y) for y in gcn4_test[\"DMS_score\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(gcn4_oh_xs_train, gcn4_oh_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(gcn4_oh_xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(gcn4_oh_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE-MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancestor/extant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dummy config with correct dimensions \n",
    "with open(\"../data/dummy_config_50_latent.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "# Top model grid search params \n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "# set up the dataloaders \n",
    "device = torch.device(\"mps\")\n",
    "gcn4_a_state_dict = \"/Users/sebs_mac/50_latent/gcn4_a/gcn4_a_r1_fold_1_model_state.pt\"\n",
    "gcn4_e_state_dict = \"/Users/sebs_mac/50_latent/gcn4_e/gcn4_e_r1_fold_1_model_state.pt\"\n",
    "\n",
    "states = [gcn4_a_state_dict, gcn4_e_state_dict]\n",
    "labels = [\"Ancestors\", \"Extants\"]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(gcn4_train, gcn4_test, states, labels, \"GCN4\",\n",
    "                                              cls_list, param_grid_list, pipe, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dummy_config.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "data_path = \"/Users/sebs_mac/uni_OneDrive/honours/data/seb_clustering_results/gcn4_seb_clusters/\"\n",
    "\n",
    "labels = [0.0, 0.02, 0.04, 0.06]\n",
    "\n",
    "states = [data_path + f\"gcn4_{prop}/gcn4_{prop}_extants_r1/gcn4_{prop}_extants_r1_model_state.pt\" for prop in labels]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(gcn4_train, gcn4_test, states, labels, \"GCN4\",\n",
    "                                               cls_list, param_grid_list, pipe, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_variants = pd.read_csv(dms_path + \"GFP_AEQVI_Sarkisyan_2016.csv\")\n",
    "gfp_variants[\"one_hot\"] = gfp_variants[\"mutated_sequence\"].apply(st.seq_to_one_hot)\n",
    "gfp_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_train, gfp_test = train_test_split(gfp_variants, train_size=0.8, random_state=42)\n",
    "gfp_train.shape, gfp_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "train_gfp_esm = get_transformer_embeddings(model, alphabet, batch_converter, gfp_train, model_layer=6)\n",
    "gfp_train = gfp_train.merge(train_mafg_esm, on=\"mutant\")\n",
    "\n",
    "test_gfp_esm = get_transformer_embeddings(model, alphabet, batch_converter, gfp_test, model_layer=6)\n",
    "gfp_test = gfp_test.merge(test_gfp_esm, on=\"mutant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data created\")\n",
    "gfp_Xs_train = np.stack(gfp_train[\"embedding\"])\n",
    "gfp_ys_train = np.array([float(y) for y in gfp_train[\"DMS_score\"]])\n",
    "\n",
    "print(\"Test data created\")\n",
    "gfp_Xs_test = np.stack(gfp_train[\"embedding\"])\n",
    "gfp_ys_test = np.array([float(y) for y in gfp_train[\"DMS_score\"]])\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(gfp_Xs_train, gfp_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(gcn4_Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(gcn4_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise what information the embeddings have captured ###\n",
    "num_pca_components = 50\n",
    "pca = PCA(num_pca_components)\n",
    "Xs_train_pca = pca.fit_transform(gfp_Xs_train)\n",
    "\n",
    "fig_dims = (7, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sc = ax.scatter(Xs_train_pca[:,0], Xs_train_pca[:,1], c=gcn4_ys_train, marker='.')\n",
    "ax.set_xlabel('PCA first principal component')\n",
    "ax.set_ylabel('PCA second principal component')\n",
    "plt.colorbar(sc, label='Variant Effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, grids = one_hot_top_model(gfp_train, gfp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFP ancestors/extant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dummy config with correct dimensions \n",
    "with open(\"../data/dummy_config_50_latent.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "a_state_dict = \"/Users/sebs_mac/50_latent/gfp_a/gfp_a_r1_wd_0.0_model_state.pt\"\n",
    "e_state_dict = \"/Users/sebs_mac/50_latent/gfp_e/gfp_e_r1_wd_0.0_model_state.pt\"\n",
    "states = [a_state_dict, e_state_dict]\n",
    "labels = [\"Ancestors\", \"Extants\"]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(gfp_train, gfp_test, states, labels, \"GFP\",\n",
    "                                              cls_list, param_grid_list, pipe, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFP Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dummy_config.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "data_path = \"/Users/sebs_mac/uni_OneDrive/honours/data/seb_clustering_results/gfp_seb_clusters/\"\n",
    "\n",
    "labels = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "states = [data_path + f\"gfp_{prop}/gfp_{prop}_extants_r1/gfp_{prop}_extants_r1_model_state.pt\" for prop in labels]\n",
    "results, grids = train_and_fit_vae_top_model(gfp_train, gfp_test, states, labels, \"GFP\",\n",
    "                                               cls_list, param_grid_list, pipe, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_variants = pd.read_csv(dms_path + \"A4_HUMAN_Seuma_2022.csv\")\n",
    "a4_variants[\"one_hot\"] = a4_variants[\"mutated_sequence\"].apply(st.seq_to_one_hot)\n",
    "a4_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_train, a4_test = train_test_split(a4_variants, train_size=0.8, random_state=42)\n",
    "a4_train.shape, a4_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "train_a4_esm = get_transformer_embeddings(model, alphabet, batch_converter, a4_train, model_layer=6)\n",
    "a4_train = a4_train.merge(train_a4_esm, on=\"mutant\")\n",
    "\n",
    "test_a4_esm = get_transformer_embeddings(model, alphabet, batch_converter, a4_test, model_layer=6)\n",
    "a4_test = a4_test.merge(test_a4_esm, on=\"mutant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a4_Xs_train = np.stack(a4_train[\"embedding\"])\n",
    "a4_ys_train = np.array([float(y) for y in a4_train[\"DMS_score\"]])\n",
    "a4_Xs_test = np.stack(a4_train[\"embedding\"])\n",
    "a4_ys_test = np.array([float(y) for y in a4_train[\"DMS_score\"]])\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(a4_Xs_train, a4_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(a4_Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(a4_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, grids = one_hot_top_model(a4_train, a4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 ancestors/extant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dummy config with correct dimensions \n",
    "with open(\"../data/dummy_config_50_latent.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "a_state_dict = \"/Users/sebs_mac/50_latent/a4_a/a4_a_r1_wd_0.0_model_state.pt\"\n",
    "e_state_dict = \"/Users/sebs_mac/50_latent/a4_e/a4_e_r1_wd_0.0_model_state.pt\"\n",
    "states = [a_state_dict, e_state_dict]\n",
    "labels = [\"Ancestors\", \"Extants\"]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(a4_train, a4_test, states, labels, \"A4\",\n",
    "                                              cls_list, param_grid_list, pipe, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dummy_config.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "data_path = \"/Users/sebs_mac/uni_OneDrive/honours/data/seb_clustering_results/a4_seb_clusters/\"\n",
    "\n",
    "labels = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "states = [data_path + f\"a4_{prop}/a4_{prop}_extants_r1/a4_{prop}_extants_r1_model_state.pt\" for prop in labels]\n",
    "results, grids = train_and_fit_vae_top_model(a4_train, a4_test, states, labels, \"A4\",\n",
    "                                               cls_list, param_grid_list, pipe, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1_variants = pd.read_csv(dms_path + \"SPG1_STRSG_Wu_2016.csv\")\n",
    "gb1_variants[\"one_hot\"] = gb1_variants[\"mutated_sequence\"].apply(st.seq_to_one_hot)\n",
    "gb1_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1_train, gb1_test = train_test_split(gb1_variants, train_size=0.8, random_state=42)\n",
    "gb1_train.shape, gb1_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "train_gb1_esm = get_transformer_embeddings(model, alphabet, batch_converter, gb1_train, model_layer=6)\n",
    "gb1_train = gb1_train.merge(train_gb1_esm, on=\"mutant\")\n",
    "\n",
    "test_gb1_esm = get_transformer_embeddings(model, alphabet, batch_converter, gb1_test, model_layer=6)\n",
    "gb1_test = gb1_test.merge(test_gb1_esm, on=\"mutant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1_Xs_train = np.stack(a4_train[\"embedding\"])\n",
    "gb1_ys_train = np.array([float(y) for y in gb1_train[\"DMS_score\"]])\n",
    "gb1_Xs_test = np.stack(a4_train[\"embedding\"])\n",
    "gb1_ys_test = np.array([float(y) for y in gb1_train[\"DMS_score\"]])\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "results, grids = perform_grid_search(gb1_Xs_train, gb1_ys_train, cls_list, param_grid_list, pipe)\n",
    "for grid in grids:\n",
    "    print(grid.best_estimator_.get_params()[\"steps\"][0][1]) # get the model details from the estimator\n",
    "    print()\n",
    "    preds = grid.predict(gb1_Xs_test)\n",
    "    print(f'{scipy.stats.spearmanr(gb1_ys_test, preds)}')\n",
    "    print('\\n', '-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, grids = one_hot_top_model(gb1_train, gb1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB1 ancestors/extant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dummy config with correct dimensions \n",
    "with open(\"../data/dummy_config_50_latent.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "a_state_dict = \"/Users/sebs_mac/50_latent/gb1_a/gb1_a_r1_wd_0.0_model_state.pt\"\n",
    "e_state_dict = \"/Users/sebs_mac/50_latent/gb1_e/gb1_e_r1_wd_0.0_model_state.pt\"\n",
    "states = [a_state_dict, e_state_dict]\n",
    "labels = [\"Ancestors\", \"Extants\"]\n",
    "\n",
    "results, grids = train_and_fit_vae_top_model(gb1_train, gb1_test, states, labels, \"GB1\",\n",
    "                                              cls_list, param_grid_list, pipe, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB1 clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dummy_config.yaml\", \"r\") as stream:\n",
    "    settings = yaml.safe_load(stream)\n",
    "\n",
    "cls_list, param_grid_list, pipe = instantiate_top_models_and_pipe()\n",
    "\n",
    "data_path = \"/Users/sebs_mac/uni_OneDrive/honours/data/seb_clustering_results/gb1_seb_clusters/\"\n",
    "\n",
    "labels = [0.0, 0.05, 0.1, 0.15]\n",
    "\n",
    "states = [data_path + f\"gb1_{prop}/gb1_{prop}_r1/gb1_{prop}_r1_model_state.pt\" for prop in labels]\n",
    "results, grids = train_and_fit_vae_top_model(gb1_train, gb1_test, states, labels, \"GB1\",\n",
    "                                               cls_list, param_grid_list, pipe, settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

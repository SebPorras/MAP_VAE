from evoVAE.models.seqVAE import SeqVAE
from evoVAE.models.types_ import *
import torch
import numpy as np


def KL_divergence(
    zMu: Tensor, zLogvar: Tensor, zSample: Tensor, seq_weights: Tensor
) -> Tensor:
    """

    KLD calculation when there is a gaussian normal distribution
    https://github.com/loschmidt/vae-dehalogenases/blob/main/scripts/VAE_model.py
    """

    zStd = zLogvar.mul(0.5).exp()
    KLD = torch.sum(0.5 * (zStd**2 + zMu**2 - 2 * torch.log(zStd) - 1), -1)

    return KLD


def KL_divergence_monte_carlo(
    zMu: Tensor, zLogvar: Tensor, zSample: Tensor, seq_weights: Tensor
) -> Tensor:
    """Based off a Monte Carlo estimation of KL divergence.
    Above batch sizes of 128 it is generally fairly accurate.
    https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed
    """

    zStd = zLogvar.mul(0.5).exp()
    p = torch.distributions.Normal(torch.zeros_like(zMu), torch.ones_like(zStd))
    q = torch.distributions.Normal(zMu, zStd)

    # 2. get the probabilities from the equation
    log_qzx = q.log_prob(zSample)
    log_pz = p.log_prob(zSample)

    # kl
    kl = log_qzx - log_pz
    kl = kl.sum(-1)
    return kl


def gaussian_likelihood(
    xHat: Tensor, globalLogSD: Tensor, x: Tensor, seq_weights: Tensor
) -> Tensor:
    """Build a distribution from parameters estimated from the decoder and the global log variance
    which is another model parameter. Calculate the log probability of the original input x under this
    distribution and return the average across all samples."""

    globalStd = globalLogSD.exp()

    qPhi = torch.distributions.Normal(loc=xHat, scale=globalStd)

    log_pxz = qPhi.log_prob(x)

    # sum up across all dims except the first
    # (https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed)
    log_pxz = log_pxz.sum(dim=tuple(range(1, log_pxz.ndim)))

    log_pxz = log_pxz * seq_weights

    return log_pxz.mean(dim=0)


def sequence_likelihood(x_one_hot: torch.Tensor, x_hat: torch.Tensor) -> Tensor:
    """
    X_hat is the categorical distribution of possible residues generated by a SeqVAE
    decoder. The likelihood of observing the original sequence under this
    distribution is calculated by summing the log probabilities of each residue.

    Parameters:
    -----------
    x_one_hot: torch.Tensor
        The one-hot encoding of the sequence to calculate the likelihood for.

    x_hat: torch.Tensor
        The categorical distribution of possible residues.

    Returns:
    --------
    log_PxGz: Tensor
        The log likelihood P_theta(x|z) of observing the original sequence under the distribution.
    """

    flat_input = torch.flatten(x_one_hot, start_dim=1)
    log_PxGz = torch.sum(flat_input * x_hat, -1)

    return log_PxGz


def elbo_importance_sampling(
    model: SeqVAE, one_hot_encoding: torch.Tensor, n_samples: int = 500
) -> Tensor:
    """
    Calculates an approximation of the marginal probability of a sequence
    using importance sampling.

    Described by Ding et al., https://www.nature.com/articles/s41467-019-13633-0

    Parameters:
    -----------
    model: SeqVAE
        The model to use for the calculation.

    one_hot_encoding: torch.Tensor
        The one-hot encoding of the sequence to calculate the marginal probability for.

    n_samples: int, optional
        The number of samples to use for the approximation (default is 500).

    Returns:
    --------
    log_elbo: Tensor

    Source:
    -------
    https://github.com/loschmidt/vae-dehalogenases/
    """

    one_hot_encoding = one_hot_encoding.expand(n_samples, -1, -1)

    log_p, z, z_mu, z_logvar = model.forward(one_hot_encoding)
    z_sigma = torch.sqrt(torch.exp(z_logvar))
    # find the random noise used for this sample
    eps = (z - z_mu) / z_sigma

    # P(x|z)
    log_PxGz = sequence_likelihood(one_hot_encoding, log_p)
    # P(z) - log probability density function for Gausian distribution
    log_Pz = torch.sum(-0.5 * z**2 - 0.5 * torch.log(2 * z.new_tensor(np.pi)), -1)
    # joint probability P(x,z)
    log_Pxz = log_PxGz + log_Pz

    # now estimate q(z|x)
    log_QzGx = torch.sum(
        -0.5 * (eps) ** 2
        - 0.5 * torch.log(2 * z.new_tensor(np.pi))
        - torch.log(z_sigma),
        -1,
    )

    # P(x,z) / Q(z|x)
    log_weight = (log_Pxz - log_QzGx).detach().data
    log_weight = log_weight.double()
    # find the max to allow for normalisation
    log_weight_max = torch.max(log_weight, 0)[0]
    # now normalise, prevents overflow when we exponentiate
    log_weight = log_weight - log_weight_max
    weight = torch.exp(log_weight)
    # average the weightings and correct the normalisation
    p_x = torch.log(torch.mean(weight, 0)) + log_weight_max

    return p_x


def frange_cycle_linear(
    n_iter, start=0.0, stop=1.0, n_cycle=4, ratio=0.5
) -> np.ndarray:
    """
    Generates a linear cyclical annealing schedule for a given number of iterations.

    This function creates a schedule that linearly increases from `start` to `stop`
    over a specified number of cycles, which can be used to prevent the issue of
    vanishing KLD (Kullback-Leibler Divergence) as described in a paper from Microsoft Research.

    Parameters:
    -----------
    n_iter : int
        Total number of iterations.
    start : float, optional
        The starting value of the schedule (default is 0.0).
    stop : float, optional
        The stopping value of the schedule (default is 1.0).
    n_cycle : int, optional
        The number of cycles to repeat the schedule (default is 4).
    ratio : float, optional
        The ratio of the increasing phase to the total period of each cycle (default is 0.5).

    Returns:
    --------
    np.ndarray
        An array of length `n_iter` containing the cyclical annealing schedule.

    Source:
    -------
    https://github.com/haofuml/cyclical_annealing
    """

    L = np.ones(n_iter) * stop
    period = n_iter / n_cycle
    step = (stop - start) / (period * ratio)  # linear schedule

    for c in range(n_cycle):
        v, i = start, 0
        while v <= stop and (int(i + c * period) < n_iter):
            L[int(i + c * period)] = v
            v += step
            i += 1

    return L
